# Projet Data POC - Modern Data Stack

Ce projet est un Proof of Concept (POC) d'une architecture de donnÃ©es moderne ("Modern Data Stack") utilisant Docker pour orchestrer l'ingestion, le stockage, la transformation et la visualisation des donnÃ©es.

## ğŸ—ï¸ Architecture des DonnÃ©es

L'architecture repose sur le paradigme **Lakehouse** utilisant Apache Iceberg.

![POC Lakehouse Architecture](images/POC%20lakehouse.drawio.png)

![Superset dashboard](images/Superset_dash.png)

```mermaid
    NiFi --> Raw
    Spark -- Read --> Raw
    Spark -- Write --> Warehouse
    Spark -- Catalog --> Postgres
    Trino -- Query --> Warehouse
    Trino -- Metadata --> Postgres
    Superset -- SQL --> Trino
```

### Ingestion & Orchestration (NiFi)

Apache NiFi est utilisÃ© pour ingÃ©rer les donnÃ©es brutes dans le bucket `raw` de MinIO.

![NiFi Flow](images/Flow%20nifi.png)

## ğŸ› ï¸ Composants de la Stack

| Composant | RÃ´le | AccÃ¨s Interface | Identifiants (DÃ©faut) |
| :--- | :--- | :--- | :--- |
| **MinIO** | Stockage S3 (Raw & Lake) | [localhost:9001](http://localhost:9001) | `minioadmin` / `minioadmin123` |
| **Apache NiFi** | Ingestion & Orchestration | [localhost:8443](https://localhost:8443) | `admin` / `admin123wisdom` |
| **PostgreSQL** | Catalogue Iceberg & Metadata | `localhost:5432` | `metadata` / `metadata` |
| **Apache Spark** | Transformation ETL (PySpark) | [localhost:8080](http://localhost:8080) | - |
| **Trino** | Moteur de requÃªtes SQL | `localhost:8081` | `admin` |
| **Apache Superset** | BI & Visualisation | [localhost:8088](http://localhost:8088) | `admin` / `admin` |

## ğŸš€ DÃ©marrage Rapide

1.  **PrÃ©-requis** : Docker et Docker Compose installÃ©s.
2.  **Configuration** : VÃ©rifiez le fichier `.env` pour les credentials de base.
3.  **Lancer la stack** :
    ```bash
    docker-compose up -d
    ```
4.  **VÃ©rifier le statut** :
    ```bash
    docker-compose ps
    ```

## ğŸ“Š Pipelines de DonnÃ©es (Spark)

Les scripts de transformation sont situÃ©s dans le dossier `/spark` :

-   **`malaria_to_iceberg.py`** : Nettoie et transforme les donnÃ©es de malaria du bucket `raw` vers la table Iceberg `demo.public.malaria`.
-   **`covid_to_iceberg.py`** : Pipeline similaire pour les donnÃ©es COVID.

**Pour exÃ©cuter un job Spark :**
```bash
docker exec -it spark spark-submit --master spark://spark:7077 /opt/spark-apps/malaria_to_iceberg.py
```

## ğŸ” Exploration des DonnÃ©es

Une fois les donnÃ©es chargÃ©es dans Iceberg, vous pouvez les requÃªter via **Trino** :
1. Connectez-vous Ã  l'interface Trino ou utilisez un client SQL.
2. RequÃªtez vos tables : `SELECT * FROM demo.public.malaria LIMIT 10;`.
3. Configurez Trino comme source de donnÃ©es dans **Superset** pour crÃ©er vos dashboards.

---
> [!NOTE]
> OpenMetadata est prÃ©sent dans le `docker-compose.yml` mais nÃ©cessite une configuration supplÃ©mentaire (Elasticsearch) pour Ãªtre pleinement fonctionnel.
